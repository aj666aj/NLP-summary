[注意力及自注意力](https://blog.csdn.net/malefactor/article/details/78767781)

[交叉熵损失CE和二值交叉熵损失BCE](https://blog.csdn.net/tsyccnh/article/details/79163834?ops_request_misc=&request_id=&biz_id=102&utm_term=%E4%BA%A4%E5%8F%89%E7%86%B5&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-79163834.nonecase&spm=1018.2226.3001.4187)

[Transformer](https://zhuanlan.zhihu.com/p/54356280)

[GPT2解析](https://www.jiqizhixin.com/articles/2019-08-26-12)

[预训练语言模型](https://zhuanlan.zhihu.com/p/254821426)

[一文了解GPT-2模型（Transformer语言模型可视化）](https://baijiahao.baidu.com/s?id=1652045261459975418&wfr=spider&for=pc)

[归一化](https://zhuanlan.zhihu.com/p/137995496)